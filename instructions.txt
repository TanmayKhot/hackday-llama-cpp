
ðŸ¥— Step-by-Step Implementation Guide for Local AI Dietary Assistant

This guide provides a comprehensive roadmap for building a personalized dietary and nutrition coach using a local LLM (Qwen2.5-3B-Instruct), integrating Retrieval-Augmented Generation (RAG) for knowledge and user data, and a robust memory system focused on tracking dietary habits and blood reports.

Phase 1: Project Setup and Environment


Step 1: Create Project Structure

Establish the following organized directory structure:
Main project folder (e.g., DietAI)
/nutrition_docs: Stores general nutrition reference files for RAG.
/user_data: Stores user-specific files (blood report summaries, preferences JSON).
/memory: Stores conversation history (short_term_memory.txt, long_term_memory.txt).
/vector_db: Stores FAISS/ChromaDB index files for RAG.
main.py: The core application script.
config.ini: Configuration settings.
requirements.txt: Project dependencies.

Step 2: Install Required Dependencies

Ensure the following Python packages are installed in your environment:
llama-cpp-python
sentence-transformers
faiss-cpu (or chromadb)
python-dotenv
Standard libraries: json, datetime, pathlib, os

Phase 2: RAG and Personal Data Integration

The RAG system is designed to provide both general nutrition facts and highly personalized context (blood reports, preferences).

Step 3: Prepare Nutrition and Personal Data

General Nutrition Knowledge: Place comprehensive documents on macronutrients, caloric content, healthy recipes, and dietary guidelines (.txt files) in /nutrition_docs.
User Preferences File: Create a structured file (e.g., user_data/preferences.json) to store key user choices:
Allergies
Dietary Style (e.g., Vegan, Keto, Paleo)
Cuisine Preferences
Stated Macro Goals (e.g., 40% Carb, 30% Protein, 30% Fat)
Blood Report Data: Create a system to input or parse key metrics (e.g., cholesterol, glucose, iron) into a summarized, contextual text format (user_data/blood_report_summary.txt) for the LLM.

Step 4: Build the Document Embedding System

Create an initial script that runs on startup/data change:
Chunking: Reads and splits files in /nutrition_docs into manageable chunks (e.g., 400 words with 50-word overlap).
Embedding: Uses an embedding model (e.g., 'all-MiniLM-L6-v2') to convert all text chunks into vectors.
Indexing: Stores these vectors in a vector database (faiss-cpu or chromadb) and saves the index in /vector_db.

Step 5: Implement Context Retrieval Function (get_context)

This function prioritizes user data before retrieving general knowledge:
Input: User query (string).
Retrieve Personal Context: Load the text content from user_data/preferences.json (formatted into a prompt string) and user_data/blood_report_summary.txt.
Retrieve General Context: Convert the user query into a vector, search the RAG index for the top 3-5 most similar nutrition text chunks.
Output: Returns a combined, structured string containing both the personal context and the retrieved general nutrition knowledge.

Phase 3: Memory Management System

The memory system tracks recent dialogue and long-term dietary progress.

Step 6: Initialize Memory Files

Ensure two text files exist in the /memory directory (create them if they do not exist):
short_term_memory.txt (for the current session, up to 10 message pairs).
long_term_memory.txt (for summaries of previous sessions).

Step 7: Build Message Counter System

Implement a simple counter in the main application loop to:
Track the number of user messages in the current conversation.
Increment the counter after each user input.
Trigger the summarization function (Step 8) when the counter reaches a threshold (e.g., 10 messages).

Step 8: Create Short-Term Memory Summarization

Build a function triggered by the message counter:
Collect the last 10 raw messages (5 user inputs, 5 assistant responses).
Send the transcript to the LLM with a specific prompt: "Summarize this conversation batch, focusing on meals discussed, any new preferences/allergies mentioned, and the recommended macro split."
Append the resulting summary and a timestamp to memory/short_term_memory.txt.
Reset the message counter to 0.

Step 9: Create Long-Term Memory System

Implement two core memory operations:
End of Conversation (/exit command):
Read all content from short_term_memory.txt.
Send to the LLM for comprehensive summarization: "Consolidate this short-term history into key long-term dietary trends, macro compliance, and persistent recommendations."
Append the comprehensive summary to memory/long_term_memory.txt with the session date.
Clear the content of memory/short_term_memory.txt.
Start of Conversation:
Read memory/long_term_memory.txt.
Extract the last 2-3 session summaries.
Format these summaries as context to be included in the new conversation's system prompt.

Phase 4: Core Application Logic


Step 10: Build the Main Conversation Loop

Structure the main.py flow:
Display welcome message and initial Dietary Disclaimer (Step 23).
Load and initialize the local LLM (Qwen2.5-3B-Instruct).
Load the embedding model and RAG index.
Load the long-term memory context (Step 9).
Enter a continuous while True loop for user input, breaking only on the /exit command.

Step 11: Implement Query Processing Pipeline

This is the core workflow for every user message:
Receive Input: Get user message. Check for special commands (Step 17).
Retrieve Context: Call get_context (Step 5) to retrieve personalized and general RAG documents.
Load Memory: Load the current session's short-term memory (raw messages or recent summary).
Construct Prompt: Build the complete prompt by combining, in order of priority:
System Prompt (Step 12)
Personal Blood Report/Preference Context
Long-Term Memory
Short-Term Memory
RAG Retrieved Documents
User Query
Generate Response: Send the prompt to the local LLM for generation (tuned for consistency: $\text{temperature} \approx 0.3-0.5$).
Update Memory Buffer: Store the user message and assistant response in a temporary buffer.
Trigger Check: Increment the message counter and check if summarization (Step 8) is needed.

Step 12: Design the System Prompt

Craft a detailed prompt that defines the AI's role and constraints:
"You are an expert Personalized Nutrition and Macro Coach. Your primary directive is to provide meal and dietary recommendations that strictly align with the user's provided blood report data and stated macro goals/preferences. Your responses must be professional, motivational, and scientifically grounded. Always include the estimated macronutrient breakdown (Carbs/Protein/Fat) for any meal recommendation. You are not a registered dietitian or a medical doctor; remind the user to consult a professional for chronic conditions."

Phase 5-8: Enhancements, Testing, and Deployment


Step 13: Add Context Window Management

Implement logic to track the total token count of the final prompt (max 2048 tokens for your model):
Prioritization (must keep): System Prompt, Blood Report Data.
Prioritization (flexible): RAG Documents, Recent Short-Term Memory, Older Long-Term Memory.
Truncate older memory entries or reduce RAG retrieval count if the token limit is exceeded.

Step 17: Add Conversation Commands

Implement the following special commands in the main.py loop:
/exit: Trigger end-of-conversation summarization and save (Step 9).
/clear: Reset current session (short_term_memory.txt) but preserve long_term_memory.txt.
/history: Display all summaries from long_term_memory.txt.
/goals: Display the current blood report summary and dietary preferences.

Step 20: Optimize Performance

Tune the system for high-quality, relevant results:
RAG: Fine-tune chunk size and overlap. Experiment with the number of retrieved documents.
LLM Parameters: Use a low temperature (e.g., $0.4$) for consistent, fact-based nutritional advice.
Indexing: Load the embedding model and vector database once at startup for speed.

Step 23: Add Dietary Disclaimer System

Implement safety features:
Disclaimer: Show a clear disclaimer at application startup stating that the assistant is an informational tool and not a substitute for a licensed healthcare professional or registered dietitian.
Emergency Guardrails: Implement keyword detection (e.g., "anorexia," "severe pain," "suicide") and immediately advise seeking professional help or emergency services.
Would you like to start by focusing on the critical RAG components (Steps 3-5) to handle the blood report and preference data?
