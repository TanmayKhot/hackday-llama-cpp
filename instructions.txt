Step-by-Step Implementation Guide for Local AI Medical Assistant
Project Overview
You're building a local medical AI assistant with RAG capabilities, conversation memory management, and persistent storage - all running on a local LLM (Qwen2.5-3B-Instruct).

The local LLM is setup in /home/tanmay/Desktop/python-projects/hackday-llama/llama-cpp-samples/models

You can refer the files installed in the virtual Environment: /home/tanmay/Desktop/python-projects/hackday-llama/venv

Phase 1: Project Setup and Environment
Step 1: Create Project Structure
Create a well-organized directory structure:

Main project folder
Subdirectory for medical documents (RAG knowledge base)
Subdirectory for memory files (short_term and long_term)
Subdirectory for vector database storage (for RAG embeddings)
Main application script file
Configuration file for settings
Requirements/dependencies file

Step 2: Install Required Dependencies
Install the following Python packages:

llama-cpp-python (already working for you)
sentence-transformers (for creating embeddings of medical documents)
faiss-cpu or chromadb (vector database for RAG implementation)
python-dotenv (for configuration management)
Standard libraries: json, datetime, pathlib, os


Phase 2: RAG Implementation Setup
Step 3: Prepare Medical Documents

Place all medical reference documents (.txt files) in your medical documents directory
Ensure documents are properly formatted and readable
Consider organizing by categories (symptoms, treatments, medications, etc.)

Step 4: Build the Document Embedding System
Create a module that:

Reads all .txt files from the medical documents directory
Splits documents into manageable chunks (200-500 words each with overlap)
Uses a local embedding model (like 'all-MiniLM-L6-v2' from sentence-transformers) to convert text chunks into vectors
Stores these embeddings in a vector database (FAISS or ChromaDB)
Creates an index that maps embeddings back to original text chunks

Step 5: Implement Document Retrieval Function
Create a function that:

Takes a user query as input
Converts the query into an embedding vector
Searches the vector database for the most similar document chunks (top 3-5 results)
Returns the relevant text passages to be used as context


Phase 3: Memory Management System
Step 6: Initialize Memory Files
Create two text files in your memory directory:

short_term_memory.txt (stores recent conversation summaries)
long_term_memory.txt (stores comprehensive conversation history)

Step 7: Build Message Counter System
Implement a mechanism to:

Track the number of user messages in the current conversation
Increment counter with each user message
Trigger summarization when counter reaches 10

Step 8: Create Short-Term Memory Summarization
Build a function that:

Collects the last 10 messages (5 user + 5 assistant pairs)
Formats them into a conversation transcript
Sends to the LLM with a specific prompt requesting a concise medical summary
Appends the summary to short_term_memory.txt with a timestamp
Resets the message counter

Step 9: Create Long-Term Memory System
Implement functions for:

End of conversation: Read all content from short_term_memory.txt, send to LLM for comprehensive summarization, append to long_term_memory.txt with session date, clear short_term_memory.txt
Start of conversation: Read long_term_memory.txt, extract recent relevant history (last 2-3 sessions), format as context for the new conversation


Phase 4: Core Application Logic
Step 10: Build the Main Conversation Loop
Create the primary application flow:

Welcome message and system initialization
Load long-term memory context at startup
Enter continuous input loop for user messages
Exit command to end conversation gracefully

Step 11: Implement Query Processing Pipeline
For each user message, create a workflow:

Receive and validate user input
Retrieve relevant medical documents using RAG (Step 5)
Load current memory context (long-term + short-term)
Construct comprehensive prompt combining: system instructions (medical expert role), memory context, RAG retrieved documents, and user query
Send to local LLM for generation
Store message pair in temporary conversation buffer
Increment message counter and check if summarization needed

Step 12: Design the System Prompt
Craft a detailed system prompt that:

Defines the AI's role as a medical advisor
Sets appropriate guardrails (not a replacement for professional medical care)
Instructs to prioritize information from provided medical documents
Specifies tone (professional, empathetic, clear)
Includes disclaimer about seeking professional medical attention for emergencies


Phase 5: Enhanced Features
Step 13: Add Context Window Management
Implement logic to:

Track total token count of the conversation context
Keep context within LLM's limits (your model has 2048 tokens)
Prioritize: system prompt > RAG documents > recent memory > older memory
Truncate older messages if needed while preserving critical medical information

Step 14: Implement Conversation Persistence
Create functionality to:

Save entire raw conversation to a dated log file when session ends
Allow users to resume previous conversations (optional)
Maintain conversation ID/session tracking

Step 15: Add Error Handling and Validation
Build robust error handling for:

Missing memory files (create if don't exist)
Empty or corrupted RAG database (rebuild prompt)
LLM generation failures (retry mechanism)
Invalid user input (validation and feedback)
File I/O errors (graceful degradation)


Phase 6: User Interface and Experience
Step 16: Create Command-Line Interface
Build a clean CLI that includes:

Clear visual separators between user and assistant messages
Command options: /exit, /clear, /help, /history
Loading indicators during RAG retrieval and LLM generation
Character counter or input validation

Step 17: Add Conversation Commands
Implement special commands:

/exit: Trigger end-of-conversation summarization and save
/clear: Reset current conversation but keep memories
/history: Display summaries from long_term_memory.txt
/help: Show available commands and usage tips


Phase 7: Testing and Optimization
Step 18: Test Each Component Individually
Verify:

RAG retrieval returns relevant medical information
Summarization produces coherent, medically accurate summaries
Memory files are properly read/written
LLM responses incorporate provided context
Message counter triggers at correct intervals

Step 19: Test End-to-End Workflows
Simulate complete user journeys:

New user with no history
Returning user with existing long-term memory
Conversation spanning multiple summarization cycles
Edge cases (very long messages, medical emergencies, unclear queries)

Step 20: Optimize Performance
Fine-tune:

RAG chunk size and overlap for better retrieval
Number of retrieved documents (balance context vs. relevance)
Summarization prompt for better quality/brevity
LLM generation parameters (temperature, top_p, max_tokens)
Vector database indexing for faster searches


Phase 8: Documentation and Deployment
Step 21: Create Configuration File
Build a config file with:

File paths (model, documents, memory, database)
LLM parameters (n_ctx, temperature, max_tokens)
RAG settings (chunk size, retrieval count)
Summarization interval (currently 10 messages)
System prompts and disclaimers

Step 22: Write User Documentation
Document:

Installation instructions
How to add medical documents
Available commands and features
Privacy and data storage explanation
Limitations and when to seek real medical help
Troubleshooting common issues

Step 23: Add Medical Disclaimer System
Implement:

Disclaimer shown at application startup
Reminder to consult healthcare professionals
Detection of emergency keywords (chest pain, suicide, severe bleeding) with immediate professional care recommendations
Ethical AI usage guidelines


Implementation Priority Order
Critical Path (Minimum Viable Product):

Steps 1-2: Environment setup
Steps 10-12: Basic conversation loop with LLM
Steps 3-5: RAG implementation
Steps 6-9: Memory management
Step 15: Basic error handling

Enhancement Phase:
6. Steps 13-14: Context management and persistence
7. Steps 16-17: Improved UI/UX
8. Steps 18-20: Testing and optimization
9. Steps 21-23: Documentation and safety features

Key Technical Considerations
RAG Best Practices:

Use semantic chunking (split by paragraphs/sections, not arbitrary character counts)
Include metadata (document title, section) with chunks
Experiment with retrieval count (3-5 documents usually optimal)
Consider hybrid search (keyword + semantic) for medical terms

Memory Management Tips:

Keep summaries concise (100-200 words per 10-message batch)
Include key medical terms, symptoms, and recommendations in summaries
Use structured format (bullet points) in memory files for easier parsing
Timestamp all entries for temporal context

LLM Integration Notes:

Your existing Llama integration will work perfectly for this
Adjust max_tokens based on response needs (200-500 for medical advice)
Consider temperature=0.3-0.5 for more consistent medical responses
Use stop sequences to prevent overly long responses

Performance Optimization:

Load embedding model and vector database once at startup (not per query)
Cache frequently retrieved documents
Use batch processing for initial document indexing
Monitor memory usage with large conversation histories

This guide provides a complete roadmap. Start with the critical path, test thoroughly, then add enhancements iteratively.